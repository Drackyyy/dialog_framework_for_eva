{
    "cdial": {
        "url": "http://115.182.62.166:5012/bot3",
        "name": "CDIAL-GPT",
        "use_history": false,
        "desc": "基于Transformer的大规模预训练语言模型极大地促进了开放领域对话的研究进展。然而目前这一技术在中文对话领域并未被广泛应用，主要原因在于目前缺乏大规模高质量的中文对话开源数据。为了推动中文对话领域的研究，弥补中文对话语料不足这一问题，清华大学交互式人工智能课题组COAI发布了一个包含1200万对话的大规模中文对话数据集LCCC，并开源了在LCCC上预训练的大规模中文对话生成模型CDial-GPT。开源地址：https://github.com/thu-coai/CDial-GPT"
    },
    "cpm": {
        "url": "http://106.12.166.20:8888/cpm_gpt_lm",
        "name": "CPM",
        "use_history": true,
        "desc": "清源 CPM (Chinese Pretrained Models) 是北京智源人工智能研究院和清华大学研究团队合作开展的大规模预训练模型开源计划，清源计划是以中文为核心的大规模预训练模型。首期开源内容包括预训练中文语言模型和预训练知识表示模型，可广泛应用于中文自然语言理解、生成任务以及知识计算应用，所有模型免费向学术界和产业界开放下载，供研究使用。论文链接：https://arxiv.org/abs/2012.00413"
    },
    "eva": {
        "url": "http://106.12.166.20:8888/eva",
        "name": "文滔",
        "use_history": true,
        "desc": "智源研究院与清华大学交互式人工智能实验室（CoAI）合作构建的全球最大的中文对话模型，其具有以下特点：<br>- 最大最优质的训练数据：悟道·文滔对话模型基于悟道对话数据集训练构建，其包含180GB高质量中文对话数据，对话总数达到1.4B，是目前全球最大的中文对话语料；同时该语料采用了高效、严格的清洗规则，使清洗后的对话数据具有很强的相关性和多样性，该规则成功从9TB原始数据中清洗得到180G高质量对话数据。<br> - 最大规模预训练中文对话模型：悟道·文滔对话模型基于24层 Transformer 的编码器-解码器框架构建，模型总参数量达到 2.8B，是全球最大的中文对话模型。为了保证训练稳定性，模型采用了超大的 Batch Size 进行训练。同时模型使用了中文的字词混合编码以适配中文的对话语料。模型在64块NVIDIA V100 GPU上训练了30天。相比于现有的中文对话模型，悟道·文滔可以生成具有更强相关性和语法性的对话回复。<br>- 广阔应用前景：悟道·文滔对话模型可以进行不受领域限制的中文对话生成，同时为进行下游对话任务（如：任务型对话、知识驱动的对话等）的研究与适配提供了坚实的基础。除此之外，业务场景下的对话系统（如：语音助手、智能音箱）也可以基于该模型开发针对性的对话功能。总体而言，悟道·文滔对话模型具有极高的研究价值和广阔的应用前景。"
    },
    "wenlan": {
        "url": "http://120.92.50.21:6175/retrieval",
        "name": "文澜·赋魂",
        "use_history": false,
        "desc": "“悟道·文澜”旨在突破基于图、文、视频相结合的多模态数据进行预训练的理论难题，生成基于大规模数据的产业级跨模态预训练模型，可完成例如看图说话、自动配图、跨模态检索等更为复杂的任务，并进一步更好地支持自动驾驶、智能辅助决策等产业级应用。 现阶段的“文澜”已初具规模，具备强大的图文检索能力和一定的常识理解能力。本页面展示的是团队开发的应用“赋魂”，可以给图片搭配精彩的文案或者符合意境的歌曲，用于互动分享。"
    },
    "wenhuiqa": {
        "url": "http://lab.aminer.cn/isoa-2021/gpt",
        "name": "文汇-问答",
        "use_history": false,
        "desc": "“悟道·文汇”旨在搭建面向认知的超大规模新型预训练模型，在多语言、多模态条件下，提升完成开放对话、基于知识的问答、可控文本生成等复杂认知推理任务的能力，使其更加接近人类水平。另外，搭建预训练模型体系，形成中文认知智能的生态，实现多样化数据和体系化模型的融合，鼓励科研机构和企业通过以脱敏数据换预训练服务等方式参与生态建设，从单点突破提升至全方位突破。本页面展示的是问答功能。"
    },
    "wenhuiqadialog": {
        "url": "http://lab.aminer.cn/isoa-2021/gpt",
        "name": "文汇-问答-模拟对话",
        "use_history": true,
        "desc": "“悟道·文汇”旨在搭建面向认知的超大规模新型预训练模型，在多语言、多模态条件下，提升完成开放对话、基于知识的问答、可控文本生成等复杂认知推理任务的能力，使其更加接近人类水平。另外，搭建预训练模型体系，形成中文认知智能的生态，实现多样化数据和体系化模型的融合，鼓励科研机构和企业通过以脱敏数据换预训练服务等方式参与生态建设，从单点突破提升至全方位突破。<br>本页面展示的通过修改输入文本格式以模拟对话的功能。"
    },
    "wenhuichat": {
        "url": "http://lab.aminer.cn/isoa-2021/gpt",
        "name": "文汇-长聊天",
        "use_history": false,
        "desc": "“悟道·文汇”旨在搭建面向认知的超大规模新型预训练模型，在多语言、多模态条件下，提升完成开放对话、基于知识的问答、可控文本生成等复杂认知推理任务的能力，使其更加接近人类水平。另外，搭建预训练模型体系，形成中文认知智能的生态，实现多样化数据和体系化模型的融合，鼓励科研机构和企业通过以脱敏数据换预训练服务等方式参与生态建设，从单点突破提升至全方位突破。本页面展示的是长聊天功能。"
    },
    "wenhuichatdialog": {
        "url": "http://lab.aminer.cn/isoa-2021/gpt",
        "name": "文汇-长聊天-模拟对话",
        "use_history": true,
        "desc": "“悟道·文汇”旨在搭建面向认知的超大规模新型预训练模型，在多语言、多模态条件下，提升完成开放对话、基于知识的问答、可控文本生成等复杂认知推理任务的能力，使其更加接近人类水平。另外，搭建预训练模型体系，形成中文认知智能的生态，实现多样化数据和体系化模型的融合，鼓励科研机构和企业通过以脱敏数据换预训练服务等方式参与生态建设，从单点突破提升至全方位突破。<br>本页面展示的通过修改输入文本格式以模拟对话的功能。"
    }
}